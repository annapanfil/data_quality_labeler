{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality labelling\n",
    "#### BDA project report\n",
    "\n",
    "Report completed by:\n",
    "* Anna Panfil\n",
    "* Igor Czudy\n",
    "* Juras Lukaševičius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dane/projekty/studia/mgr2_UPV/data_quality_labeler/dataset_creator.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append(self.data.iloc[self.data.sample(frac=duplicate_percentage).index, :])\n"
     ]
    }
   ],
   "source": [
    "# Setting directory\n",
    "\n",
    "import os\n",
    "os.chdir('D:/Users/Vartotojas/Documents/GitHub/data_quality_labeler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "\n",
    "from dataset_creator import FakeDataset, MISSING_SYMBOLS\n",
    "\n",
    "filename = 'dataset.csv'\n",
    "OUTLIER_PERCENTAGE = 0.1\n",
    "DUPLICATE_PERCENTAGE = 0.15\n",
    "MISSING_PERCENTAGE = 0.1\n",
    "\n",
    "\n",
    "dataset = FakeDataset(dataset_size = 100)\\\n",
    "        .add_dominated_string_column(dominated_percentage=0.9)\\\n",
    "        .add_mishmashed_case(mishmashed_percentage=0.1)\\\n",
    "        .add_outliers_above(outlier_percentage = OUTLIER_PERCENTAGE)\\\n",
    "        .add_duplicates(duplicate_percentage = DUPLICATE_PERCENTAGE)\\\n",
    "        .add_missing(missing_percentage = MISSING_PERCENTAGE)\\\n",
    "        .to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 78,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>results1</th>\n",
       "      <th>results2</th>\n",
       "      <th>category</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
       "      <td>Anthony</td>\n",
       "      <td>Hampton</td>\n",
       "      <td>1970-04-13</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2189765068820564</td>\n",
       "      <td>C</td>\n",
       "      <td>rodriguezjonathan@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Odom</td>\n",
       "      <td>1991-11-22</td>\n",
       "      <td>11</td>\n",
       "      <td>0.38665489707246736</td>\n",
       "      <td>C</td>\n",
       "      <td>rhondamccoy@example.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cameron</td>\n",
       "      <td>Meadows</td>\n",
       "      <td>1950-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6289607608312131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>13</td>\n",
       "      <td>none</td>\n",
       "      <td>C</td>\n",
       "      <td>zgarner@example.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veronica</td>\n",
       "      <td>Sullivan</td>\n",
       "      <td>1974-09-24</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2368428069939834</td>\n",
       "      <td>B</td>\n",
       "      <td>ashelton@example.net</td>\n",
=======
       "      <td>Michele</td>\n",
       "      <td>Parker</td>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>553</td>\n",
       "      <td>none</td>\n",
       "      <td>A</td>\n",
       "      <td>elizabethwilson@example.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dana</td>\n",
       "      <td>Cunningham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NONE</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.23356275907386193</td>\n",
       "      <td>A</td>\n",
       "      <td>gsimmons@example.com</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ariana</td>\n",
       "      <td>Weiss</td>\n",
       "      <td>2015-09-28</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.1893010411447136</td>\n",
       "      <td>b</td>\n",
       "      <td>johnwilliams@example.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1958-01-11</td>\n",
       "      <td>93</td>\n",
       "      <td>-0.6320832828351748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>salasjohn@example.com</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "       name   surname   birthdate results1             results2 category  \\\n",
       "0   Anthony   Hampton  1970-04-13       23   0.2189765068820564        C   \n",
       "1     Brian      Odom  1991-11-22       11  0.38665489707246736        C   \n",
       "2   Cameron   Meadows  1950-05-13        3   0.6289607608312131      NaN   \n",
       "3      Jack   Johnson  1974-12-21       13                 none        C   \n",
       "4  Veronica  Sullivan  1974-09-24      100   0.2368428069939834        B   \n",
       "\n",
       "                           email  \n",
       "0  rodriguezjonathan@example.com  \n",
       "1        rhondamccoy@example.org  \n",
       "2                            NaN  \n",
       "3            zgarner@example.net  \n",
       "4           ashelton@example.net  "
      ]
     },
     "execution_count": 3,
=======
       "      name     surname   birthdate results1             results2 category  \\\n",
       "0  Michele      Parker  2010-01-20      553                 none        A   \n",
       "1     Dana  Cunningham         NaN       24                  NaN     none   \n",
       "2  Jessica       Lopez         NaN       99  0.23356275907386193        A   \n",
       "3   Ariana       Weiss  2015-09-28       10  -1.1893010411447136        b   \n",
       "4  Barbara     Johnson  1958-01-11       93  -0.6320832828351748      NaN   \n",
       "\n",
       "                         email gender  \n",
       "0  elizabethwilson@example.com    NaN  \n",
       "1                         NONE      F  \n",
       "2         gsimmons@example.com      F  \n",
       "3     johnwilliams@example.org    NaN  \n",
       "4        salasjohn@example.com    NaN  "
      ]
     },
     "execution_count": 78,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 79,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset_scores = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 80,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(MISSING_SYMBOLS, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 81,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         12\n",
       "surname      12\n",
       "birthdate    11\n",
       "results1     12\n",
       "results2     12\n",
       "category     12\n",
       "email        12\n",
       "gender       12\n",
       "dtype: int64"
      ]
     },
<<<<<<< HEAD
     "execution_count": 9,
=======
     "execution_count": 81,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data._convert(numeric=True, datetime=True).convert_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"missing_percentage\"] = data.isna().sum().sum()/data.size\n",
    "dataset_scores[\"most_missing_column\"] = data.isna().sum().max()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(dataset_scores[\"missing_percentage\"],2) == MISSING_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      553.0\n",
       "1       24.0\n",
       "2       99.0\n",
       "3       10.0\n",
       "4       93.0\n",
       "       ...  \n",
       "110     40.0\n",
       "111     38.0\n",
       "112    587.0\n",
       "113     24.0\n",
       "114     47.0\n",
       "Name: results1, Length: 115, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.results1.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"duplication_percentage\"] = sum(data.duplicated())/ data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check outliers\n",
    "\n",
    "For numerical values we use the same method as in box plot (outlier is more tham q3 + 1.5 IQR or less than q1 - 1.5 IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "string_cols = data.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "outliers_nums = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    \n",
    "    iqr = q3-q1\n",
    "    \n",
    "    upper_bound = q3 + (1.5*iqr)\n",
    "    lower_bound = q1 - (1.5*iqr)\n",
    "\n",
    "    outliers_nums.append(np.sum((data[col] > upper_bound) | (data[col] < lower_bound)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For string columns we look for rare values (less than 5% of the observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in string_cols:\n",
    "    if not len(data[col].unique())/len(data[col]) > 0.5 and\\\n",
    "            (rare := data[\"category\"].value_counts().min()/data.shape[0]) < 0.05: # rare category\n",
    "        outliers_nums.append(rare)\n",
    "\n",
    "dataset_scores[\"outliers_percentage\"] = sum(outliers_nums)/data.size\n",
    "dataset_scores[\"most_outliers_column\"] = max(outliers_nums)/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look for dominant values (more than 80% of the observations in column) and columns with unique values (eg. id, email), which may be not useful in further predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:  \n",
    "    if len(data[col].unique())/len(data[col]) > 0.5: # column with rather unique values\n",
    "        dataset_scores[\"unique_columns\"] += 1      \n",
    "    if data[col].value_counts().max()/data.shape[0] > 0.8: # dominant category\n",
    "        dataset_scores[\"dominated_columns\"] += 1\n",
    "        \n",
    "dataset_scores[\"dominated_columns\"] /= len(data.columns)\n",
    "dataset_scores[\"unique_columns\"] /= len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mishmashed formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mishmashed_cases = []\n",
    "for col in string_cols:\n",
    "    unique_in_data = len(data[\"category\"].unique())\n",
    "    truly_unique = len(data[\"category\"].map(lambda x: x.lower() if not pd.isna(x) else x).unique())\n",
    "\n",
    "    mishmashed_cases.append((unique_in_data - truly_unique)/truly_unique)\n",
    "\n",
    "dataset_scores[\"max_mishmashed_case\"] = max(mishmashed_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas: correlation, not good dates, mishmashed formats, upper and lower cased, duplicates, is it actual, are all values the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'missing_percentage': 0.10326086956521739,\n",
       "             'most_missing_column': 0.10434782608695652,\n",
       "             'duplication_percentage': 0.017391304347826087,\n",
       "             'outliers_percentage': 0.013100189035916825,\n",
       "             'most_outliers_column': 0.10434782608695652,\n",
       "             'unique_columns': 0.75,\n",
       "             'dominated_columns': 0.125,\n",
       "             'max_mishmashed_case': 0.75})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"missing_percentage\": 10, # many missing values is difficult to handle\n",
    "    \"most_missing_column\": 2, # if 1 we had a column with huge amount of missing values, we'd have to drop it\n",
    "    \"duplication_percentage\": 4, # many duplicates means less data\n",
    "    \"outliers_percentage\": 2, # outliers may be removed or cause problems with predictions\n",
    "    \"most_outliers_column\": 1,\n",
    "    \"unique_columns\": 5, # if all columns are unique, we can't do much with it\n",
    "    \"dominated_columns\": 3, # if a column has one dominant category, it may be not very useful\n",
    "    \"max_mishmashed_case\": 1 # our data may be dirty and require a lot of cleaning\n",
    "}\n",
    "\n",
    "assert weights.keys() == dataset_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = 0\n",
    "for name, score in dataset_scores.items():\n",
    "    final_score += score * weights[name]\n",
    "\n",
    "final_score /= sum(weights.values())\n",
    "final_score = 1 - final_score # 1 is the best score, 0 – the worst\n",
    "final_score\n",
    "\n",
    "dataset_scores[\"dataset_quality_score\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for formatting"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if columns only with numbers. If so, they are formatted as float64\n",
    "# IMPORTANT! If the column is only numerical and an identifier, its name must be\n",
    "# listed below. Otherwise, it will be added into outlier calculation.\n",
    "\n",
    "categorical_variables = ['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame:\n",
      "        name   surname   birthdate  results1  results2 category  \\\n",
      "0    Anthony   Hampton  1970-04-13      23.0  0.218977        C   \n",
      "1      Brian      Odom  1991-11-22      11.0  0.386655        C   \n",
      "2    Cameron   Meadows  1950-05-13       3.0  0.628961      NaN   \n",
      "3       Jack   Johnson  1974-12-21      13.0       NaN        C   \n",
      "4   Veronica  Sullivan  1974-09-24     100.0  0.236843        B   \n",
      "..       ...       ...         ...       ...       ...      ...   \n",
      "95      Anne   Andrews  1925-01-28      36.0  0.014766        A   \n",
      "96   William     Brown  1964-08-11       6.0  0.445439        C   \n",
      "97   Kenneth     Baker  1998-07-05      78.0  0.919524        B   \n",
      "98      Carl  Fletcher  2012-09-09       5.0  0.657009        A   \n",
      "99     Tracy       NaN         NaN       7.0       NaN        C   \n",
      "\n",
      "                            email  \n",
      "0   rodriguezjonathan@example.com  \n",
      "1         rhondamccoy@example.org  \n",
      "2                             NaN  \n",
      "3             zgarner@example.net  \n",
      "4            ashelton@example.net  \n",
      "..                            ...  \n",
      "95       parkerdennis@example.net  \n",
      "96                            NaN  \n",
      "97             alex72@example.com  \n",
      "98             vjones@example.net  \n",
      "99       cummingsgreg@example.net  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "string_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert string columns to numeric if they contain only numbers or 'NaN'\n",
    "for col in string_columns:\n",
    "    try:\n",
    "        # Skip conversion for columns in list A\n",
    "        if col in categorical_variables:\n",
    "            continue\n",
    "            \n",
    "        # Check if there are any numbers in the column\n",
    "        if pd.to_numeric(data[col], errors='coerce').notna().any():\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        else:\n",
    "            # If no numbers found, keep the column as an object\n",
    "            data[col] = data[col].astype('object')\n",
    "    except ValueError:\n",
    "        print(f\"Unable to convert column '{col}' to numeric.\")\n",
    "\n",
    "# Check the result\n",
    "print(\"Updated DataFrame:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['results1', 'results2'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we select the numeric columns in the data frame\n",
    "\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for outlier detection using descriptive statistics (quantiles)\n",
    "\n",
    "def identify_outliers(column):\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return column[(column < lower_bound) | (column > upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers in each numeric column\n",
    "\n",
    "outliers_dict = {col: identify_outliers(data[col]) for col in numeric_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential number of outliers in results1:\n",
      "0\n",
      "\n",
      "\n",
      "Potential number of outliers in results2:\n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_values = len(data[list(outliers_dict.keys())])\n",
    "outlier_count = 0\n",
    "\n",
    "for col, outliers in outliers_dict.items():\n",
    "        print(f\"Potential number of outliers in {col}:\")\n",
    "        print(len(outliers))\n",
    "        print(\"\\n\")\n",
    "        outlier_count = len(outliers) + outlier_count\n",
    "\n",
    "if numerical_values == 0:\n",
    "    dataset_scores[\"outlier_percentage\"] = 0\n",
    "else:\n",
    "    dataset_scores[\"outlier_percentage\"] = outlier_count/numerical_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate detection (Lower case/higher case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name   Day\n",
      "0     John  noOn\n",
      "1     Mary  nOon\n",
      "2      Bob  Noon\n",
      "3      Bob  noon\n",
      "4     john  NOon\n",
      "5     mary  noon\n",
      "6      BOB  noON\n",
      "7   Thomas  noon\n",
      "8   thomas  nOON\n",
      "9   THOMAS  NOON\n",
      "10  Thomas  noon\n",
      "11     Bob  NoOn\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "info = {'Name': ['John', 'Mary', 'Bob', 'Bob', 'john', 'mary', 'BOB', 'Thomas', 'thomas', 'THOMAS', 'Thomas', 'Bob'], \n",
    "       'Day': ['noOn', 'nOon', 'Noon', 'noon', 'NOon', 'noon', 'noON', 'noon', 'nOON', 'NOON', 'noon', 'NoOn']}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column 'name' has 10 truly similar values differing only by capitalization.\n",
      "List of case-sensitive duplicate values: ['Anthony', 'Anthony', 'Cameron', 'Cameron', 'Kimberly', 'Kimberly', 'Kyle', 'Kyle', 'John', 'John', 'James', 'James', 'Matthew', 'Matthew', 'Matthew', 'Matthew', 'Anne', 'Anne', 'Bradley', 'Bradley', 'Victoria', 'Victoria']\n",
      "\n",
      "\n",
      "Unique values: ['Kimberly', 'James', 'John', 'Anthony', 'Victoria', 'Cameron', 'Matthew', 'Anne', 'Bradley', 'Kyle']\n",
      "\n",
      "\n",
      "The column 'surname' has 7 truly similar values differing only by capitalization.\n",
      "List of case-sensitive duplicate values: ['Morris', 'Morris', 'Williams', 'Williams', 'Williams', 'Gonzalez', 'Gonzalez', 'Jones', 'Jones', 'Jones', 'Jones', 'Allen', 'Allen', 'Lewis', 'Lewis', 'Brown', 'Brown', 'Brown']\n",
      "\n",
      "\n",
      "Unique values: ['Gonzalez', 'Jones', 'Williams', 'Morris', 'Brown', 'Allen', 'Lewis']\n",
      "\n",
      "\n",
      "The column 'birthdate' does not have truly similar values differing only by capitalization.\n",
      "\n",
      "\n",
      "The column 'category' has 3 truly similar values differing only by capitalization.\n",
      "List of case-sensitive duplicate values: ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n",
      "\n",
      "\n",
      "Unique values: ['A', 'C', 'B']\n",
      "\n",
      "\n",
      "The column 'email' does not have truly similar values differing only by capitalization.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_unique_val = 0\n",
    "n_variants = 0\n",
    "\n",
    "def get_case_duplicates(column):\n",
    "    \"\"\"\n",
    "    Get the list of similar values (ignoring NaN) that only differ by capitalization in a column.\n",
    "    \n",
    "    Parameters:\n",
    "    - column: pandas Series, the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - has_duplicates: True if case-insensitive duplicates are found, False otherwise\n",
    "    - count_duplicates: count of similar values differing only by capitalization\n",
    "    - duplicate_values: list of similar values differing only by capitalization\n",
    "    \"\"\"\n",
    "    lowercased_values = column.dropna().astype(str).str.lower()\n",
    "    \n",
    "    def are_truly_case_duplicates(val1, val2):\n",
    "        return val1 != val2 and val1.lower() == val2.lower()\n",
    "    \n",
    "    duplicated_mask = lowercased_values.duplicated(keep=False)\n",
    "    has_duplicates = duplicated_mask.any()\n",
    "    \n",
    "    count_duplicates = 0\n",
    "    duplicate_values = []\n",
    "    \n",
    "    for val in lowercased_values[duplicated_mask].unique():\n",
    "        similar_values = lowercased_values[lowercased_values == val].index.tolist()\n",
    "        if len(similar_values) > 1 and not any(are_truly_case_duplicates(lowercased_values[i], lowercased_values[j]) for i in similar_values for j in similar_values if i != j):\n",
    "            count_duplicates += 1\n",
    "            duplicate_values.extend(similar_values)\n",
    "    \n",
    "    return has_duplicates, count_duplicates, column[duplicate_values].tolist()\n",
    "\n",
    "object_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "for column_name in object_columns:\n",
    "    has_duplicates, count_duplicates, duplicate_values = get_case_duplicates(data[column_name])\n",
    "\n",
    "    if has_duplicates:\n",
    "        print(f\"The column '{column_name}' has {count_duplicates} truly similar values differing only by capitalization.\")\n",
    "        print(f\"List of case-sensitive duplicate values: {duplicate_values}\")\n",
    "        unique_values = set(duplicate_values)\n",
    "        unique_list = list(unique_values)\n",
    "        print('\\n')\n",
    "        print(f\"Unique values: {unique_list}\")\n",
    "        \n",
    "        n_unique_val = n_unique_val + count_duplicates\n",
    "        n_variants = n_variants + len(unique_list)\n",
    "        \n",
    "    else:\n",
    "        print(f\"The column '{column_name}' does not have truly similar values differing only by capitalization.\")\n",
    "    print(\"\\n\")  # Add a separator for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_variants == 0:\n",
    "    dataset_scores[\"duplicate_proportion\"] = 0\n",
    "else:\n",
    "    dataset_scores[\"duplicate_proportion\"] = 1 - n_unique_val/n_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores[\"duplicate_proportion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating badges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
=======
   "execution_count": 94,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [],
   "source": [
    "# save obtained scores to json\n",
    "import json\n",
    " \n",
    "filename=\"./badge_data.json\"\n",
    "json_object = json.dumps(dataset_scores, indent=4)\n",
    " \n",
    "with open(filename, \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 95,
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add badges paste this to your readme.md file:\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.missing_percentage&label=missing_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_missing_column&label=most_missing_column)\n",
<<<<<<< HEAD
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.outlier_percentage&label=outlier_percentage)\n"
=======
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.duplication_percentage&label=duplication_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.outliers_percentage&label=outliers_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_outliers_column&label=most_outliers_column)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.unique_columns&label=unique_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dominated_columns&label=dominated_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.max_mishmashed_case&label=max_mishmashed_case)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dataset_quality_score&label=dataset_quality_score)\n"
>>>>>>> 9fc3a00571b47eeba5e4a6a4da606f933cdf15c8
     ]
    }
   ],
   "source": [
    "repo_url = \"https://github.com/annapanfil/data_quality_labeler\" #todo: get dinamically\n",
    "\n",
    "ownername = repo_url.split(\"/\")[3]\n",
    "repo_name = repo_url.split(\"/\")[4]\n",
    "branch = \"main\"\n",
    "\n",
    "\n",
    "print(\"To add badges paste this to your readme.md file:\")\n",
    "for badge in dataset_scores.keys():\n",
    "    print(f\"![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2F{ownername}%2F{repo_name}%2F{branch}%2F{filename}&query=%24.{badge}&label={badge})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### SIDE CODE (maybe useful?)\n",
    "\n",
    "## This converts all object columns into lowercase strings\n",
    "\n",
    "# List of object columns to convert\n",
    "object_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert object columns to lowercase strings, skipping NaN values\n",
    "for col in object_columns:\n",
    "    data[col] = data[col].apply(lambda x: str(x).lower() if pd.notna(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This is an example data frane for the capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "info = {'Name': ['John', 'Mary', 'Bob', 'Bob', 'john', 'mary', 'BOB', 'Thomas', 'thomas', 'THOMAS', 'Thomas', 'Bob'], \n",
    "       'Day': ['noOn', 'nOon', 'Noon', 'noon', 'NOon', 'noon', 'noON', 'noon', 'nOON', 'NOON', 'noon', 'NoOn']}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
