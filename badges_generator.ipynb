{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality labelling\n",
    "#### BDA project report\n",
    "\n",
    "Report completed by:\n",
    "* Anna Panfil\n",
    "* Igor Czudy\n",
    "* Juras Lukaševičius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "import os\n",
    "os.chdir('D:/Users/Vartotojas/Documents/GitHub/data_quality_labeler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "\n",
    "from dataset_creator import FakeDataset, MISSING_SYMBOLS\n",
    "\n",
    "filename = 'dataset.csv'\n",
    "OUTLIER_PERCENTAGE = 0.1\n",
    "DUPLICATE_PERCENTAGE = 0.15\n",
    "MISSING_PERCENTAGE = 0.1\n",
    "\n",
    "\n",
    "dataset = FakeDataset(dataset_size = 100)\\\n",
    "        .add_dominated_string_column(dominated_percentage=0.9)\\\n",
    "        .add_mishmashed_case(mishmashed_percentage=0.1)\\\n",
    "        .add_outliers_above(outlier_percentage = OUTLIER_PERCENTAGE)\\\n",
    "        .add_duplicates(duplicate_percentage = DUPLICATE_PERCENTAGE)\\\n",
    "        .add_missing(missing_percentage = MISSING_PERCENTAGE)\\\n",
    "        .to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>results1</th>\n",
       "      <th>results2</th>\n",
       "      <th>category</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samantha</td>\n",
       "      <td>Martinez</td>\n",
       "      <td>1918-09-14</td>\n",
       "      <td>85</td>\n",
       "      <td>0.33799399624313065</td>\n",
       "      <td>A</td>\n",
       "      <td>david70@example.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chelsey</td>\n",
       "      <td>Gillespie</td>\n",
       "      <td>1944-02-19</td>\n",
       "      <td>41</td>\n",
       "      <td>1.1028054198064805</td>\n",
       "      <td>C</td>\n",
       "      <td>chentammy@example.org</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>Hall</td>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>56</td>\n",
       "      <td>2.3695139681554642</td>\n",
       "      <td>A</td>\n",
       "      <td>aramirez@example.net</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andrew</td>\n",
       "      <td>Owens</td>\n",
       "      <td>1988-07-17</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.6611189327249538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jhunt@example.org</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lauren</td>\n",
       "      <td>King</td>\n",
       "      <td>2012-05-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1390247312700723</td>\n",
       "      <td>A</td>\n",
       "      <td>donald13@example.org</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name    surname   birthdate results1             results2 category  \\\n",
       "0  Samantha   Martinez  1918-09-14       85  0.33799399624313065        A   \n",
       "1   Chelsey  Gillespie  1944-02-19       41   1.1028054198064805        C   \n",
       "2     Tyler       Hall  2023-12-19       56   2.3695139681554642        A   \n",
       "3    Andrew      Owens  1988-07-17       21  -0.6611189327249538      NaN   \n",
       "4    Lauren       King  2012-05-09      NaN   1.1390247312700723        A   \n",
       "\n",
       "                   email gender  \n",
       "0    david70@example.com    NaN  \n",
       "1  chentammy@example.org      F  \n",
       "2   aramirez@example.net      F  \n",
       "3      jhunt@example.org      M  \n",
       "4   donald13@example.org      F  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset_scores = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(MISSING_SYMBOLS, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         12\n",
       "surname      12\n",
       "birthdate    12\n",
       "results1     12\n",
       "results2     12\n",
       "category     12\n",
       "email        12\n",
       "gender       12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data._convert(numeric=True, datetime=True).convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"missing_percentage\"] = data.isna().sum().sum()/data.size\n",
    "dataset_scores[\"most_missing_column\"] = data.isna().sum().max()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(dataset_scores[\"missing_percentage\"],2) == MISSING_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDEA: We can check how easy it is to train the model to replace missing values (looking at the error).\n",
    "For all columns with nan % > sth. Random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"duplication_percentage\"] = sum(data.duplicated())/ data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check outliers\n",
    "\n",
    "For numerical values we use the same method as in box plot (outlier is more tham q3 + 1.5 IQR or less than q1 - 1.5 IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "string_cols = data.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "outliers_nums = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    \n",
    "    iqr = q3-q1\n",
    "    \n",
    "    upper_bound = q3 + (1.5*iqr)\n",
    "    lower_bound = q1 - (1.5*iqr)\n",
    "\n",
    "    outliers_nums.append(np.sum((data[col] > upper_bound) | (data[col] < lower_bound)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For string columns we look for rare values (less than 5% of the observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in string_cols:\n",
    "    if not len(data[col].unique())/len(data[col]) > 0.5 and\\\n",
    "            (rare := data[\"category\"].value_counts().min())/data.shape[0] < 0.05: # rare category\n",
    "        outliers_nums.append(rare)\n",
    "\n",
    "dataset_scores[\"outliers_percentage\"] = sum(outliers_nums)/data.size\n",
    "dataset_scores[\"most_outliers_column\"] = max(outliers_nums)/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look for dominant values (more than 80% of the observations in column) and columns with unique values (eg. id, email), which may be not useful in further predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:  \n",
    "    if len(data[col].unique())/len(data[col]) > 0.5: # column with rather unique values\n",
    "        dataset_scores[\"unique_columns\"] += 1\n",
    "    if data[col].value_counts().max()/data.shape[0] > 0.8: # dominant category\n",
    "        dataset_scores[\"dominated_columns\"] += 1\n",
    "        \n",
    "dataset_scores[\"dominated_columns\"] /= len(data.columns)\n",
    "dataset_scores[\"unique_columns\"] /= len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mishmashed formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mishmashed_cases = []\n",
    "for col in string_cols:\n",
    "    unique_in_data = len(data[\"category\"].unique())\n",
    "    truly_unique = len(data[\"category\"].map(lambda x: x.lower() if not pd.isna(x) else x).unique())\n",
    "\n",
    "    mishmashed_cases.append((unique_in_data - truly_unique)/truly_unique)\n",
    "\n",
    "dataset_scores[\"max_mishmashed_case\"] = max(mishmashed_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IDEAS: other measurements about this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas: correlation, not good dates, rules provided by user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'missing_percentage': 0.10434782608695652,\n",
       "             'most_missing_column': 0.10434782608695652,\n",
       "             'duplication_percentage': 0.017391304347826087,\n",
       "             'outliers_percentage': 0.01847826086956522,\n",
       "             'most_outliers_column': 0.09565217391304348,\n",
       "             'unique_columns': 0.75,\n",
       "             'dominated_columns': 0.125,\n",
       "             'max_mishmashed_case': 0.75})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"missing_percentage\": 10,   # many missing values is difficult to handle\n",
    "    \"most_missing_column\": 2,   # if 1 we had a column with huge amount of missing values, we'd have to drop it\n",
    "    \"duplication_percentage\": 4,# many duplicates means less data\n",
    "    \"outliers_percentage\": 2,   # outliers may be removed or cause problems with predictions\n",
    "    \"most_outliers_column\": 1,\n",
    "    \"unique_columns\": 5,        # if all columns are unique, we can't do much with it\n",
    "    \"dominated_columns\": 3,     # if a column has one dominant category, it may be not very useful\n",
    "    \"max_mishmashed_case\": 1    # our data may be dirty and require a lot of cleaning\n",
    "}\n",
    "\n",
    "assert weights.keys() == dataset_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = 0\n",
    "for name, score in dataset_scores.items():\n",
    "    final_score += score * weights[name]\n",
    "\n",
    "final_score /= sum(weights.values())\n",
    "final_score = 1 - final_score # 1 is the best score, 0 – the worst\n",
    "final_score\n",
    "\n",
    "dataset_scores[\"dataset_quality_score\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if columns only with numbers. If so, they are formatted as float64\n",
    "# IMPORTANT! If the column is only numerical and an identifier, its name must be\n",
    "# listed below. Otherwise, it will be added into outlier calculation.\n",
    "\n",
    "categorical_variables = ['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame:\n",
      "            name    surname   birthdate  results1  results2 category  \\\n",
      "0       Samantha   Martinez  1918-09-14        85  0.337994        A   \n",
      "1        Chelsey  Gillespie  1944-02-19        41  1.102805        C   \n",
      "2          Tyler       Hall  2023-12-19        56  2.369514        A   \n",
      "3         Andrew      Owens  1988-07-17        21 -0.661119     <NA>   \n",
      "4         Lauren       King  2012-05-09      <NA>  1.139025        A   \n",
      "..           ...        ...         ...       ...       ...      ...   \n",
      "110       Thomas      Adams  1943-06-10         0 -0.302413        A   \n",
      "111       Angela      Foley  1936-01-16        20 -0.555290        C   \n",
      "112        Linda     Lester  1995-05-27        79 -0.069751        B   \n",
      "113  Christopher     Harris  2013-05-15        25 -0.658892        C   \n",
      "114          Jon       Rich  1979-12-22        68  1.367867        A   \n",
      "\n",
      "                            email gender  \n",
      "0             david70@example.com   <NA>  \n",
      "1           chentammy@example.org      F  \n",
      "2            aramirez@example.net      F  \n",
      "3               jhunt@example.org      M  \n",
      "4            donald13@example.org      F  \n",
      "..                            ...    ...  \n",
      "110  katherinegriffin@example.com      F  \n",
      "111         rrobinson@example.org      F  \n",
      "112        greendavid@example.com      F  \n",
      "113         brandon88@example.org      F  \n",
      "114            seth60@example.com      F  \n",
      "\n",
      "[115 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "string_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert string columns to numeric if they contain only numbers or 'NaN'\n",
    "for col in string_columns:\n",
    "    try:\n",
    "        # Skip conversion for columns in list A\n",
    "        if col in categorical_variables:\n",
    "            continue\n",
    "            \n",
    "        # Check if there are any numbers in the column\n",
    "        if pd.to_numeric(data[col], errors='coerce').notna().any():\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        else:\n",
    "            # If no numbers found, keep the column as an object\n",
    "            data[col] = data[col].astype('object')\n",
    "    except ValueError:\n",
    "        print(f\"Unable to convert column '{col}' to numeric.\")\n",
    "\n",
    "# Check the result\n",
    "print(\"Updated DataFrame:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['results1', 'results2'], dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we select the numeric columns in the data frame\n",
    "\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for outlier detection using descriptive statistics (quantiles)\n",
    "\n",
    "def identify_outliers(column):\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return column[(column < lower_bound) | (column > upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers in each numeric column\n",
    "\n",
    "outliers_dict = {col: identify_outliers(data[col]) for col in numeric_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential number of outliers in results1:\n",
      "11\n",
      "\n",
      "\n",
      "Potential number of outliers in results2:\n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_values = len(data[list(outliers_dict.keys())])\n",
    "outlier_count = 0\n",
    "\n",
    "for col, outliers in outliers_dict.items():\n",
    "        print(f\"Potential number of outliers in {col}:\")\n",
    "        print(len(outliers))\n",
    "        print(\"\\n\")\n",
    "        outlier_count = len(outliers) + outlier_count\n",
    "\n",
    "if numerical_values == 0:\n",
    "    dataset_scores[\"outlier_percentage\"] = 0\n",
    "else:\n",
    "    dataset_scores[\"outlier_percentage\"] = outlier_count/numerical_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate detection (Lower case/higher case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name   Day\n",
      "0     John  noOn\n",
      "1     Mary  nOon\n",
      "2      Bob  Noon\n",
      "3      Bob  noon\n",
      "4     john  NOon\n",
      "5     mary  noon\n",
      "6      BOB  noON\n",
      "7   Thomas  noon\n",
      "8   thomas  nOON\n",
      "9   THOMAS  NOON\n",
      "10  Thomas  noon\n",
      "11     Bob  NoOn\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "info = {'Name': ['John', 'Mary', 'Bob', 'Bob', 'john', 'mary', 'BOB', 'Thomas', 'thomas', 'THOMAS', 'Thomas', 'Bob'], \n",
    "       'Day': ['noOn', 'nOon', 'Noon', 'noon', 'NOon', 'noon', 'noON', 'noon', 'nOON', 'NOON', 'noon', 'NoOn']}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_val = 0\n",
    "n_variants = 0\n",
    "\n",
    "def get_case_duplicates(column):\n",
    "    \"\"\"\n",
    "    Get the list of similar values (ignoring NaN) that only differ by capitalization in a column.\n",
    "    \n",
    "    Parameters:\n",
    "    - column: pandas Series, the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - has_duplicates: True if case-insensitive duplicates are found, False otherwise\n",
    "    - count_duplicates: count of similar values differing only by capitalization\n",
    "    - duplicate_values: list of similar values differing only by capitalization\n",
    "    \"\"\"\n",
    "    lowercased_values = column.dropna().astype(str).str.lower()\n",
    "    \n",
    "    def are_truly_case_duplicates(val1, val2):\n",
    "        return val1 != val2 and val1.lower() == val2.lower()\n",
    "    \n",
    "    duplicated_mask = lowercased_values.duplicated(keep=False)\n",
    "    has_duplicates = duplicated_mask.any()\n",
    "    \n",
    "    count_duplicates = 0\n",
    "    duplicate_values = []\n",
    "    \n",
    "    for val in lowercased_values[duplicated_mask].unique():\n",
    "        similar_values = lowercased_values[lowercased_values == val].index.tolist()\n",
    "        if len(similar_values) > 1 and not any(are_truly_case_duplicates(lowercased_values[i], lowercased_values[j]) for i in similar_values for j in similar_values if i != j):\n",
    "            count_duplicates += 1\n",
    "            duplicate_values.extend(similar_values)\n",
    "    \n",
    "    return has_duplicates, count_duplicates, column[duplicate_values].tolist()\n",
    "\n",
    "object_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "for column_name in object_columns:\n",
    "    has_duplicates, count_duplicates, duplicate_values = get_case_duplicates(data[column_name])\n",
    "\n",
    "    if has_duplicates:\n",
    "        print(f\"The column '{column_name}' has {count_duplicates} truly similar values differing only by capitalization.\")\n",
    "        print(f\"List of case-sensitive duplicate values: {duplicate_values}\")\n",
    "        unique_values = set(duplicate_values)\n",
    "        unique_list = list(unique_values)\n",
    "        print('\\n')\n",
    "        print(f\"Unique values: {unique_list}\")\n",
    "        \n",
    "        n_unique_val = n_unique_val + count_duplicates # number of actually unique values\n",
    "        n_variants = n_variants + len(unique_list) # number of these unique value variants (with different capitalization)\n",
    "        \n",
    "    else:\n",
    "        print(f\"The column '{column_name}' does not have truly similar values differing only by capitalization.\")\n",
    "    print(\"\\n\")  # Add a separator for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_variants == 0:\n",
    "    dataset_scores[\"duplicate_proportion\"] = 0\n",
    "else:\n",
    "    dataset_scores[\"duplicate_proportion\"] = 1 - n_unique_val/n_variants  # the lower the score, the less duplicates there are - 0 is a better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores[\"duplicate_proportion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This badge mainly shows that there is evidence of documentation, but can't be certain of its validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dest = 'D:/Users/Vartotojas/Documents/GitHub/data_quality_labeler/documentation.txt' # destination of documentation\n",
    "\n",
    "with open(doc_dest, 'r') as file:\n",
    "    content = file.read().lower() # Lowercase to avoid inconsistensies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strings = []\n",
    "\n",
    "variable_definitions_synonyms = [[\n",
    "    \"variable definitions\",\n",
    "    \"variable declarations\",\n",
    "    \"variable assignments\",\n",
    "    \"declaration of variables\",\n",
    "    \"defining variables\",\n",
    "    \"variable initialization\",\n",
    "    \"variable setup\",\n",
    "    \"variable specification\",\n",
    "    \"variable establishment\",\n",
    "    \"variable creation\",\n",
    "    \"variable naming\",\n",
    "    \"setting variables\",\n",
    "    \"variable instantiation\",\n",
    "    \"variable initialization\",\n",
    "    \"defining data elements\",\n",
    "    \"variable configuration\"\n",
    "]]\n",
    "\n",
    "variable_formatting_constraints_synonyms = [[\n",
    "    \"variable formatting constraints\",\n",
    "    \"formatting restrictions for variables\",\n",
    "    \"constraints on variable formatting\",\n",
    "    \"variable format limitations\",\n",
    "    \"rules for formatting variables\",\n",
    "    \"constraints for variable presentation\",\n",
    "    \"formatting guidelines for variables\",\n",
    "    \"variable presentation constraints\",\n",
    "    \"variable formatting rules\",\n",
    "    \"limitations on variable format\",\n",
    "    \"formatting specifications for variables\",\n",
    "    \"variable format constraints\",\n",
    "    \"variable styling constraints\",\n",
    "    \"variable appearance rules\"\n",
    "]]\n",
    "\n",
    "author_synonyms = [[\n",
    "    \"author\",\n",
    "    \"creator\",\n",
    "    \"writer\",\n",
    "    \"originator\",\n",
    "    \"contributor\",\n",
    "    \"developer\",\n",
    "    \"composer\",\n",
    "    \"maker\",\n",
    "    \"designer\",\n",
    "    \"architect\",\n",
    "    \"producer\",\n",
    "    \"editor\",\n",
    "    \"compiler\",\n",
    "    \"engineer\",\n",
    "    \"craftsman\",\n",
    "    \"builder\"\n",
    "]]\n",
    "\n",
    "date_synonyms = [[\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "    \"timestamp\",\n",
    "    \"day\",\n",
    "    \"moment\",\n",
    "    \"occasion\",\n",
    "    \"event time\",\n",
    "    \"calendar date\",\n",
    "    \"point in time\",\n",
    "    \"temporal\",\n",
    "    \"chronology\",\n",
    "    \"timeframe\",\n",
    "    \"period\",\n",
    "    \"date and time\",\n",
    "    \"timing\",\n",
    "    \"schedule\"\n",
    "]]\n",
    "\n",
    "place_synonyms = [[\n",
    "    \"place\",\n",
    "    \"location\",\n",
    "    \"spot\",\n",
    "    \"site\",\n",
    "    \"position\",\n",
    "    \"area\",\n",
    "    \"region\",\n",
    "    \"locale\",\n",
    "    \"venue\",\n",
    "    \"setting\",\n",
    "    \"point\",\n",
    "    \"site\",\n",
    "    \"geographic location\",\n",
    "    \"positioning\",\n",
    "    \"spot\",\n",
    "    \"address\"\n",
    "]]\n",
    "\n",
    "allowable_ranges_synonyms = [[\n",
    "    \"allowable ranges\",\n",
    "    \"permissible intervals\",\n",
    "    \"acceptable spans\",\n",
    "    \"allowed boundaries\",\n",
    "    \"permitted scopes\",\n",
    "    \"acceptable extent\",\n",
    "    \"valid ranges\",\n",
    "    \"tolerable limits\",\n",
    "    \"authorized variations\",\n",
    "    \"acceptable thresholds\",\n",
    "    \"sanctioned intervals\",\n",
    "    \"legitimate scopes\",\n",
    "    \"admissible spans\",\n",
    "    \"approved boundaries\",\n",
    "    \"permissible ranges\",\n",
    "    \"allowed margins\"\n",
    "]]\n",
    "\n",
    "rules_synonyms = [[\n",
    "    \"rules\",\n",
    "    \"guidelines\",\n",
    "    \"instructions\",\n",
    "    \"regulations\",\n",
    "    \"policies\",\n",
    "    \"procedures\",\n",
    "    \"directives\",\n",
    "    \"standards\",\n",
    "    \"protocols\",\n",
    "    \"requirements\",\n",
    "    \"specifications\",\n",
    "    \"criteria\",\n",
    "    \"conditions\",\n",
    "    \"principles\",\n",
    "    \"laws\",\n",
    "    \"norms\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strings = variable_definitions_synonyms + variable_formatting_constraints_synonyms + rules_synonyms + author_synonyms + date_synonyms + place_synonyms + allowable_ranges_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found string: variable definitions\n",
      "Found string: rules\n",
      "Found string: author\n",
      "Found string: moment\n",
      "Found string: place\n",
      "Found string: allowable ranges\n"
     ]
    }
   ],
   "source": [
    "context = 0\n",
    "\n",
    "for search_string in search_strings:\n",
    "    for i in search_string:\n",
    "        if i in content:\n",
    "            print(f\"Found string: {i}\")\n",
    "            context = context + 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores[\"documentation_detail\"] = context / len(search_strings) # 1 is the best score, 0 is the worst\n",
    "dataset_scores[\"documentation_detail\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating badges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save obtained scores to json\n",
    "import json\n",
    " \n",
    "filename=\"./badge_data.json\"\n",
    "json_object = json.dumps(dataset_scores, indent=4)\n",
    " \n",
    "with open(filename, \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add badges paste this to your readme.md file:\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.missing_percentage&label=missing_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_missing_column&label=most_missing_column)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.duplication_percentage&label=duplication_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.outliers_percentage&label=outliers_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_outliers_column&label=most_outliers_column)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.unique_columns&label=unique_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dominated_columns&label=dominated_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.max_mishmashed_case&label=max_mishmashed_case)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dataset_quality_score&label=dataset_quality_score)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.outlier_percentage&label=outlier_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.duplicate_proportion&label=duplicate_proportion)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.documentation_detail&label=documentation_detail)\n"
     ]
    }
   ],
   "source": [
    "repo_url = \"https://github.com/annapanfil/data_quality_labeler\" #todo: get dinamically\n",
    "\n",
    "ownername = repo_url.split(\"/\")[3]\n",
    "repo_name = repo_url.split(\"/\")[4]\n",
    "branch = \"main\"\n",
    "\n",
    "\n",
    "print(\"To add badges paste this to your readme.md file:\")\n",
    "for badge in dataset_scores.keys():\n",
    "    print(f\"![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2F{ownername}%2F{repo_name}%2F{branch}%2F{filename}&query=%24.{badge}&label={badge})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### SIDE CODE (maybe useful?)\n",
    "\n",
    "## This converts all object columns into lowercase strings\n",
    "\n",
    "# List of object columns to convert\n",
    "object_columns = data.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert object columns to lowercase strings, skipping NaN values\n",
    "for col in object_columns:\n",
    "    data[col] = data[col].apply(lambda x: str(x).lower() if pd.notna(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This is an example data frane for the capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "info = {'Name': ['John', 'Mary', 'Bob', 'Bob', 'john', 'mary', 'BOB', 'Thomas', 'thomas', 'THOMAS', 'Thomas', 'Bob'], \n",
    "       'Day': ['noOn', 'nOon', 'Noon', 'noon', 'NOon', 'noon', 'noON', 'noon', 'nOON', 'NOON', 'noon', 'NoOn']}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
