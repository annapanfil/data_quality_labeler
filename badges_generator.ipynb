{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dane/projekty/studia/mgr2_UPV/data_quality_labeler/dataset_creator.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append(self.data.iloc[self.data.sample(frac=duplicate_percentage).index, :])\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "\n",
    "from dataset_creator import FakeDataset, MISSING_SYMBOLS\n",
    "\n",
    "filename = 'dataset.csv'\n",
    "OUTLIER_PERCENTAGE = 0.1\n",
    "DUPLICATE_PERCENTAGE = 0.15\n",
    "MISSING_PERCENTAGE = 0.1\n",
    "\n",
    "\n",
    "dataset = FakeDataset(dataset_size = 100)\\\n",
    "        .add_dominated_string_column(dominated_percentage=0.9)\\\n",
    "        .add_mishmashed_case(mishmashed_percentage=0.1)\\\n",
    "        .add_outliers_above(outlier_percentage = OUTLIER_PERCENTAGE)\\\n",
    "        .add_duplicates(duplicate_percentage = DUPLICATE_PERCENTAGE)\\\n",
    "        .add_missing(missing_percentage = MISSING_PERCENTAGE)\\\n",
    "        .to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>surname</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>results1</th>\n",
       "      <th>results2</th>\n",
       "      <th>category</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michele</td>\n",
       "      <td>Parker</td>\n",
       "      <td>2010-01-20</td>\n",
       "      <td>553</td>\n",
       "      <td>none</td>\n",
       "      <td>A</td>\n",
       "      <td>elizabethwilson@example.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dana</td>\n",
       "      <td>Cunningham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NONE</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.23356275907386193</td>\n",
       "      <td>A</td>\n",
       "      <td>gsimmons@example.com</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ariana</td>\n",
       "      <td>Weiss</td>\n",
       "      <td>2015-09-28</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.1893010411447136</td>\n",
       "      <td>b</td>\n",
       "      <td>johnwilliams@example.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1958-01-11</td>\n",
       "      <td>93</td>\n",
       "      <td>-0.6320832828351748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>salasjohn@example.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     surname   birthdate results1             results2 category  \\\n",
       "0  Michele      Parker  2010-01-20      553                 none        A   \n",
       "1     Dana  Cunningham         NaN       24                  NaN     none   \n",
       "2  Jessica       Lopez         NaN       99  0.23356275907386193        A   \n",
       "3   Ariana       Weiss  2015-09-28       10  -1.1893010411447136        b   \n",
       "4  Barbara     Johnson  1958-01-11       93  -0.6320832828351748      NaN   \n",
       "\n",
       "                         email gender  \n",
       "0  elizabethwilson@example.com    NaN  \n",
       "1                         NONE      F  \n",
       "2         gsimmons@example.com      F  \n",
       "3     johnwilliams@example.org    NaN  \n",
       "4        salasjohn@example.com    NaN  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset_scores = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(MISSING_SYMBOLS, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         12\n",
       "surname      12\n",
       "birthdate    11\n",
       "results1     12\n",
       "results2     12\n",
       "category     12\n",
       "email        12\n",
       "gender       12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data._convert(numeric=True, datetime=True).convert_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"missing_percentage\"] = data.isna().sum().sum()/data.size\n",
    "dataset_scores[\"most_missing_column\"] = data.isna().sum().max()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(dataset_scores[\"missing_percentage\"],2) == MISSING_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      553.0\n",
       "1       24.0\n",
       "2       99.0\n",
       "3       10.0\n",
       "4       93.0\n",
       "       ...  \n",
       "110     40.0\n",
       "111     38.0\n",
       "112    587.0\n",
       "113     24.0\n",
       "114     47.0\n",
       "Name: results1, Length: 115, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.results1.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores[\"duplication_percentage\"] = sum(data.duplicated())/ data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check outliers\n",
    "\n",
    "For numerical values we use the same method as in box plot (outlier is more tham q3 + 1.5 IQR or less than q1 - 1.5 IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "string_cols = data.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "outliers_nums = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    \n",
    "    iqr = q3-q1\n",
    "    \n",
    "    upper_bound = q3 + (1.5*iqr)\n",
    "    lower_bound = q1 - (1.5*iqr)\n",
    "\n",
    "    outliers_nums.append(np.sum((data[col] > upper_bound) | (data[col] < lower_bound)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For string columns we look for rare values (less than 5% of the observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in string_cols:\n",
    "    if not len(data[col].unique())/len(data[col]) > 0.5 and\\\n",
    "            (rare := data[\"category\"].value_counts().min()/data.shape[0]) < 0.05: # rare category\n",
    "        outliers_nums.append(rare)\n",
    "\n",
    "dataset_scores[\"outliers_percentage\"] = sum(outliers_nums)/data.size\n",
    "dataset_scores[\"most_outliers_column\"] = max(outliers_nums)/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look for dominant values (more than 80% of the observations in column) and columns with unique values (eg. id, email), which may be not useful in further predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:  \n",
    "    if len(data[col].unique())/len(data[col]) > 0.5: # column with rather unique values\n",
    "        dataset_scores[\"unique_columns\"] += 1      \n",
    "    if data[col].value_counts().max()/data.shape[0] > 0.8: # dominant category\n",
    "        dataset_scores[\"dominated_columns\"] += 1\n",
    "        \n",
    "dataset_scores[\"dominated_columns\"] /= len(data.columns)\n",
    "dataset_scores[\"unique_columns\"] /= len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mishmashed formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mishmashed_cases = []\n",
    "for col in string_cols:\n",
    "    unique_in_data = len(data[\"category\"].unique())\n",
    "    truly_unique = len(data[\"category\"].map(lambda x: x.lower() if not pd.isna(x) else x).unique())\n",
    "\n",
    "    mishmashed_cases.append((unique_in_data - truly_unique)/truly_unique)\n",
    "\n",
    "dataset_scores[\"max_mishmashed_case\"] = max(mishmashed_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas: correlation, not good dates, mishmashed formats, upper and lower cased, duplicates, is it actual, are all values the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'missing_percentage': 0.10326086956521739,\n",
       "             'most_missing_column': 0.10434782608695652,\n",
       "             'duplication_percentage': 0.017391304347826087,\n",
       "             'outliers_percentage': 0.013100189035916825,\n",
       "             'most_outliers_column': 0.10434782608695652,\n",
       "             'unique_columns': 0.75,\n",
       "             'dominated_columns': 0.125,\n",
       "             'max_mishmashed_case': 0.75})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"missing_percentage\": 10, # many missing values is difficult to handle\n",
    "    \"most_missing_column\": 2, # if 1 we had a column with huge amount of missing values, we'd have to drop it\n",
    "    \"duplication_percentage\": 4, # many duplicates means less data\n",
    "    \"outliers_percentage\": 2, # outliers may be removed or cause problems with predictions\n",
    "    \"most_outliers_column\": 1,\n",
    "    \"unique_columns\": 5, # if all columns are unique, we can't do much with it\n",
    "    \"dominated_columns\": 3, # if a column has one dominant category, it may be not very useful\n",
    "    \"max_mishmashed_case\": 1 # our data may be dirty and require a lot of cleaning\n",
    "}\n",
    "\n",
    "assert weights.keys() == dataset_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = 0\n",
    "for name, score in dataset_scores.items():\n",
    "    final_score += score * weights[name]\n",
    "\n",
    "final_score /= sum(weights.values())\n",
    "final_score = 1 - final_score # 1 is the best score, 0 â€“ the worst\n",
    "final_score\n",
    "\n",
    "dataset_scores[\"dataset_quality_score\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create badges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save obtained scores to json\n",
    "import json\n",
    " \n",
    "filename=\"./badge_data.json\"\n",
    "json_object = json.dumps(dataset_scores, indent=4)\n",
    " \n",
    "with open(filename, \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add badges paste this to your readme.md file:\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.missing_percentage&label=missing_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_missing_column&label=most_missing_column)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.duplication_percentage&label=duplication_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.outliers_percentage&label=outliers_percentage)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.most_outliers_column&label=most_outliers_column)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.unique_columns&label=unique_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dominated_columns&label=dominated_columns)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.max_mishmashed_case&label=max_mishmashed_case)\n",
      "![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fannapanfil%2Fdata_quality_labeler%2Fmain%2F./badge_data.json&query=%24.dataset_quality_score&label=dataset_quality_score)\n"
     ]
    }
   ],
   "source": [
    "repo_url = \"https://github.com/annapanfil/data_quality_labeler\" #todo: get dinamically\n",
    "\n",
    "ownername = repo_url.split(\"/\")[3]\n",
    "repo_name = repo_url.split(\"/\")[4]\n",
    "branch = \"main\"\n",
    "\n",
    "\n",
    "print(\"To add badges paste this to your readme.md file:\")\n",
    "for badge in dataset_scores.keys():\n",
    "    print(f\"![DQ Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2F{ownername}%2F{repo_name}%2F{branch}%2F{filename}&query=%24.{badge}&label={badge})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
